{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rsVZXyrR_YS"
      },
      "source": [
        "# Introduction\n",
        "You will write functions to implement evaluation metrics for classification and regression problems. You will:\n",
        " - Use library functions from scikit-learn (https://scikit-learn.org/stable/)\n",
        " - Use NumPy and matplotlib\n",
        " - Write and call functions in Python\n",
        " - Gain understanding of the evaluation metrics used.\n",
        "\n",
        "\n",
        "Scikit-learn (https://scikit-learn.org/stable/) is a Python library with a wide range of ML algorithms. We will be using some of these algorithms during this course, but we will also be looking at the principles behind the algorithms in order to understand these rather than simply applying functions from libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9hvPsUKR_YU"
      },
      "source": [
        "# 0. Preliminaries\n",
        "We firstly import NumPy and matplotlib as we will be using these throughout the worksheet. We use a 'magic' function `%matplotlib inline` to display plots in the worksheet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK2v5K3KR_YU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LmZsCc3R_YU"
      },
      "source": [
        "# 1. Classification\n",
        "In this question you will use a toy dataset from scikit-learn. You will use functions from scikit-learn to load the data, divide it into training and testing sets, and then fit a simple classifier to the training set. You will then write functions to calculate accuracy, precision, and recall. Finally, you will check your functions against the functions from scikit-learn.\n",
        "\n",
        "## Part a) Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxHRoXywR_YU"
      },
      "outputs": [],
      "source": [
        "# scikit-learn comes with a number of toy datasets (https://sklearn.org/datasets/index.html#toy-datasets)\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the wine dataset from sklearn. You may want to take a look at the format of the dataset\n",
        "wine = datasets.load_wine()\n",
        "\n",
        "# Save the datapoints into the variable X and the targets into the variable y\n",
        "X = wine.data\n",
        "y = wine.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeiGd13AR_YV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4HLSL9IR_YV"
      },
      "source": [
        "Take a look at the target values in y. What do you notice about these? Why are these suitable for a classification algorithm rather than a regression algorithm?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQMnaYzNR_YV"
      },
      "outputs": [],
      "source": [
        "#  Look at the values in y\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-nJc_7hR_YV"
      },
      "source": [
        "## Part b) Divide the data into training and testing sets\n",
        "Use the function `train_test_split` from `sklearn.model_selection` to split out the data and targets into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbxcR3TmR_YV"
      },
      "outputs": [],
      "source": [
        "# We import the function train_test_split from sklearn and use this to split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# The function returns splits of each array passed in.\n",
        "# The proportion to be used as the training set is given by test_size\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZic8I6JR_YV"
      },
      "outputs": [],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enR0XtNgR_YV"
      },
      "source": [
        "## Part c) Import the k-nearest neighbours classifier and run it on the data\n",
        "Scikit-learn has a huge range of *estimators* that you can use with your dataset. An estimator is any procedure that can be used to fit data and make predictions from it. Here we will import the k-nearest neighbours classifier, instantiate it, run it on our training set, and then use it to generate some predictions. You will learn more about k-nearest neighbours in Week 14. For now, we are simply using it to generate some predictions.\n",
        "\n",
        "The general procedure for using the estimators in scikit-learn is as follows. Every estimator has a method `fit(X, y)` and a method `predict(T)`.\n",
        "\n",
        "1) Import the estimator\n",
        "    e.g. `from sklearn.models import Classifier`\n",
        "    \n",
        "2) Instantiate the estimator to a variable\n",
        "    e.g. `est = Classifier(hparams)`\n",
        "    \n",
        "3) Fit the estimator to the data\n",
        "    e.g. `est.fit(X, y)`\n",
        "    \n",
        "4) Make a prediction\n",
        "    e.g. `predictions = est.predict(test_data)`\n",
        "    \n",
        "You can see an example of this in the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "4OOKRcIVR_YV"
      },
      "outputs": [],
      "source": [
        "# We first import the classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# We instantiate the classifier with 5 neighbours\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# We fit the model using our training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Finally, we generate predictions on the test data\n",
        "ypred_test=knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI5fgRMNR_YV"
      },
      "source": [
        "## Part d) Evaluating the classifier\n",
        "In this section we will implement functions for accuracy, precision and recall, and compare them with the functions given in sklearn (they should give the same results!)\n",
        "\n",
        "The wine dataset has 3 classes. We will write functions to compute the accuracy of the classifer, the macro-averaged precision and the macro-averaged recall.\n",
        "\n",
        "Recall the equations for accuracy, precision, and recall:\n",
        "\n",
        "$$Accuracy = \\frac{\\text{Number correct}}{\\text{Total datapoints}}$$\n",
        "i.e. the number of correctly classified datapoints as a proportion of all $n$ datapoints\n",
        "\n",
        "$$Precision_c = \\frac{TP_c}{TP_c+FP_c}$$\n",
        "i.e. the precision for class $c$ is the number of true positives for class $c$ as a proportion of the total number of positive predictions for class $c$\n",
        "\n",
        "$$Recall_c = \\frac{TP_c}{TP_c+FN_c}$$\n",
        "i.e. the recall for class $c$ is the number of true positives for class $c$ as a proportion of the total number of actual positives for class $c$\n",
        "\n",
        "The macro-averaged precision and macro-averaged recall are then simply calculated by averaging the precision (or recall) for each class:\n",
        "\n",
        "$$Precision = \\frac{1}{k} \\sum_{c = 1}^k Precision_c, \\quad Recall = \\frac{1}{k} \\sum_{c = 1}^k Recall_c$$\n",
        "\n",
        "We can automatically generate the confusion matrix for our data using the function `confusion_matrix` from `sklearn.metrics`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYG_H60mR_YV"
      },
      "outputs": [],
      "source": [
        "# Import the function confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Build the confusion matrix from the target test set y_test and our predicted values ypred_test\n",
        "cm = confusion_matrix(y_test, ypred_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pswPuFVPR_YV"
      },
      "source": [
        "Take a look at the confusion matrix. What should its dimensions be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXdjy4YYR_YV"
      },
      "outputs": [],
      "source": [
        "# Look at the confusion matrix cm\n",
        "plt.matshow(cm)\n",
        "cm\n",
        "\n",
        "# Dimensions should be 3 by 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km6t40kmR_YV"
      },
      "source": [
        "(**Optional**) Write a function `my_accuracy` that takes in two arrays `y` for target values and `pred` for predicted  values, and returns accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvhGoGpeR_YV"
      },
      "outputs": [],
      "source": [
        "def my_accuracy(y, pred):\n",
        "    # Write your answer here\n",
        "    cm = confusion_matrix(y, pred)\n",
        "    acc = np.diag(cm).sum()/cm.sum()\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYbaFcftR_YW"
      },
      "source": [
        "(**Optional**) Write a function `my_recall_macro` that takes in two arrays `y` for target values and `pred` for predicted  values, and returns recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N-aV7WvR_YW"
      },
      "outputs": [],
      "source": [
        "def my_recall_macro(y, pred):\n",
        "    recalls = []\n",
        "    cm = confusion_matrix(y, pred)\n",
        "    # Write your answer here\n",
        "    for i, row in enumerate(cm):\n",
        "        recall_i = row[i]/row.sum()\n",
        "        recalls.append(recall_i)\n",
        "    return np.mean(recalls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2PlMSyVR_YW"
      },
      "source": [
        "(**Optional**) Write a function `my_precision_macro` that takes in two arrays `y` for target values and `pred` for predicted  values, and returns precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BdW2r-2R_YW"
      },
      "outputs": [],
      "source": [
        "def my_precision_macro(y, pred):\n",
        "    # Write your answer here\n",
        "    precs = []\n",
        "    cm = confusion_matrix(y, pred)\n",
        "    for i, col in enumerate(cm.T):\n",
        "        prec_i = col[i]/col.sum()\n",
        "        precs.append(prec_i)\n",
        "    return np.mean(precs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Ase_loR_YW"
      },
      "source": [
        "(**Optional**) Check that your functions match those in sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2nIsOqiR_YW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "my_accuracy(y_test, ypred_test) == accuracy_score(y_test, ypred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a_8_KhkR_YW"
      },
      "outputs": [],
      "source": [
        "my_recall_macro(y_test, ypred_test)==recall_score(y_test, ypred_test, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOGbDESeR_YW"
      },
      "outputs": [],
      "source": [
        "my_precision_macro(y_test, ypred_test)==precision_score(y_test, ypred_test, average='macro')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}