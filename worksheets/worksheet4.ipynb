{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709798ac-dec7-4b53-96e0-8f53aef9943f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This worksheet focuses on Neural Networks. You will:\n",
    "\n",
    "- Implement your own version of a Single Layer Perceptron (SLP) to understand the underlying mechanics, and compare it with an equivalent implementation built using `PyTorch`.\n",
    "\n",
    "- Build and train Multi‑Layer Perceptrons (MLPs) in `PyTorch` for both classification and regression tasks, exploring how to design, configure, and optimise these neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d83e2d-1586-423a-b714-f801f4274883",
   "metadata": {},
   "source": [
    "# 0. Preliminaries\n",
    "We firstly import NumPy and matplotlib as we will be using these throughout the worksheet. We use a function %matplotlib inline to display plots in the worksheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71c918-0892-4a9d-8658-3587a9c38bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: import NumPy and matplotlib here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf28900-f6ad-4c65-aad3-ecee6da74f91",
   "metadata": {},
   "source": [
    "# 1. Single Layer Perceptron\n",
    "In this question, you will build and train a single‑layer perceptron using PyTorch to make predictions on the breast cancer dataset.\n",
    "This is a binary classification task where the goal is to classify each instance as malignant or benign based on 30 numerical features extracted from medical images.\n",
    "\n",
    "In this question, you will:\n",
    "\n",
    "(a) Load the breast cancer dataset from  `sklearn` and store the features and targets in suitable variables.\\\n",
    "(b) Separate your data into a training and test split.\\\n",
    "(c) (Optional) Write your own function to implement Single Layer Perceptron.\\\n",
    "(d) Implement Single Layer Perceptron in `PyTorch` (e.g., a single `nn.Linear` layer with a `sigmoid` output).\\\n",
    "    Train your PyTorch perceptron on the training data using an appropriate loss function and an optimizer.\\\n",
    "(e) Evaluate the performance of both models on the test data using appropriate metrics (e.g., accuracy, precision).     \n",
    "(f) Plot the confusion matrix to visualise the performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f655f-c936-4721-8779-405758483517",
   "metadata": {},
   "source": [
    "## Part (a)  \n",
    "Import the package `datasets` from `sklearn` and then load the load_breast_cancer dataset (function is `load_breast_cancer()`). Save the data into a variable `X` and the targets into a variable `Y`.  \n",
    "Take a look at the data in `X`. How many datapoints are there? How many features does each datapoint have? (Hint: use `np.shape`).  \n",
    "Take a look at the targets. Is this suitable for a classification algorithm or a regression algorithm?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6b443-b2bb-4970-9f68-8c7562c64a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: import suitable packages, load the dataset, and save data and targets into variables X and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8039d8-560c-48c8-b74a-f787670df0f5",
   "metadata": {},
   "source": [
    "## Part (b)\n",
    "\n",
    "Use the function `train_test_split` from `sklearn.model_selection` to split your data into a training set and a held-out test set. Use a test set that is 0.2 of the original dataset. Set the parameter `random_state` to 10 to help with replication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cba23-4941-4b54-9cb2-7381ab937975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import the package train_test_split from sklearn.model_selection.\n",
    "\n",
    "# Split the dataset into Xtr, Xtest, Ytr, Ytest\n",
    "Xtr, Xtest, Ytr, Ytest=##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5cc832-9012-416b-87c4-cb55e0c10fd0",
   "metadata": {},
   "source": [
    "Perceptrons are very sensitive to feature scale. Standardise your input data by applying `StandardScaler` from `sklearn.preprocessing` to both the training set (`Xtr`) and the test set (`Xtest`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6830ae2-2da3-41d4-8b08-0867d435de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xtr = scaler.fit_transform(Xtr)\n",
    "Xtest = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e838b50-06fc-4685-bf35-2f5a848c215b",
   "metadata": {},
   "source": [
    "## (Optional) Part (c)  \n",
    "Recall from the lecture that a single-layer perceptron runs as follows:  \n",
    "\n",
    "**Training step**:  \n",
    "- For each training datapoint $(\\vec{x}_i)$:  \n",
    "  - Compute the linear combination $(z = \\vec{w} \\cdot \\vec{x}_i + b)$.  \n",
    "  - Pass $(z)$ through the activation function (sigmoid function in this case) to get the predicted class $(y_{\\text{pred}})$.  \n",
    "  - Compute the error as $(e = y_i - y_{\\text{pred}})$, where $(y_i)$ is the true label.  \n",
    "  - Update the weights and bias using the perceptron learning rule:  \n",
    "    $[\n",
    "    \\vec{w} \\gets \\vec{w} + \\eta \\cdot e \\cdot \\vec{x}_i  \n",
    "    ]  \n",
    "    [\n",
    "    b \\gets b + \\eta \\cdot e\n",
    "    ]$  \n",
    "  Here, $(\\eta)$ is the learning rate.  \n",
    "\n",
    "**Prediction step**:  \n",
    "- For a given datapoint $(\\vec{x})$:  \n",
    "  - Compute the linear combination $(z = \\vec{w} \\cdot \\vec{x} + b)$.  \n",
    "  - Pass $(z)$ through the step function to obtain the class prediction.  \n",
    "\n",
    "Write function(s) to implement the training and prediction steps. Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ceea72-e0de-4950-8a15-b7427a223c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerPerceptron:\n",
    "    def __init__(self, input_size, learning_rate, iterat):\n",
    "        #TODO# initialise the weights to random values and set the bias to 0\n",
    "        self.weights = ##TODO## (HINT: use np.random.rand())\n",
    "        self.bias = ##TODO## \n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterat = iterat\n",
    "\n",
    "    def activation(self, z):\n",
    "        #TO DO # Write a function to implement the **sigmoid activation function**. \n",
    "        return ##TODO## \n",
    "   \n",
    "    def train(self, X, y):\n",
    "        for epoch in range(self.iterat):\n",
    "            for i in range(X.shape[0]):\n",
    "                # Calculate the linear combination\n",
    "                z = np.dot(X[i], self.weights) + self.bias\n",
    "                y_pred = self.activation(z)\n",
    "\n",
    "                #TODO# Calculate error between target and predicted values\n",
    "                error = ##TODO## \n",
    "                \n",
    "                #TODO# update the weights and bias according to the above equations\n",
    "                self.weights += ##TODO##\n",
    "                self.bias += ##TODO##\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.activation(z)\n",
    "#Train the perceptron\n",
    "input_size = Xtr.shape[1] # To pass the number of features\n",
    "perceptron1 = SingleLayerPerceptron(input_size=input_size, learning_rate=0.01, iterat=10)\n",
    "\n",
    "##TODO##  #Train the perceptron with the Train data\n",
    "perceptron1.train(Xtr, Ytr)\n",
    "\n",
    "#Test the perceptron\n",
    "my_y_pred_prob = ##TODO##\n",
    "my_Ypred = (my_y_pred_prob >= 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e63eaf-5cf0-4765-af45-213d74b5fb3e",
   "metadata": {},
   "source": [
    "## Part (d) Perceptron in PyTorch\n",
    "In this step, you will build a simple neural network model that implements a single‑layer perceptron in PyTorch. A perceptron consists of one fully‑connected (linear) layer that computes a weighted sum of the inputs, followed by an activation function that maps this value to a predicted class. For binary classification, we apply a sigmoid activation, which converts the linear output into a probability between 0 and 1.\n",
    "\n",
    "### Fully-connected layer\n",
    "A fully‑connected layer connects every input feature to every output unit. In PyTorch, this is implemented using the `nn.Linear` class, which performs the linear transformation  $(z = \\vec{w} \\cdot \\vec{x}_i + b)$ before the activation function is applied.\n",
    "\n",
    "Documentation: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "### Activation Functions\n",
    "PyTorch does not include activation functions inside `nn.Linear`, so they must be added explicitly. For a perceptron used in binary classification, we apply `torch.sigmoid()` to the output of the linear layer. This produces a probability that can be thresholded to obtain a class label.\n",
    "\n",
    "Forward Pass\n",
    "In PyTorch, the `forward()` method defines how data flows through the model. Inputs are passed through the linear layer, then through the activation function, and the final output is returned. This computation graph determines how gradients are calculated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c2613-7a1f-4d77-ab62-0bba4421c843",
   "metadata": {},
   "source": [
    "Let's import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f0aa6-dbc9-4fe3-860a-7357da650f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ea37be-f3f8-420e-979c-38bd88d312df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronTorch(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7763ada-a03b-4fcb-9ec9-4ac9154d10d7",
   "metadata": {},
   "source": [
    "After desigining our network, we need to create a training function to calculate the loss for each input and perform backpropagation to optimise the network.\n",
    "During training, the weights of all the layers will be updated.\n",
    "\n",
    "Below, we build a training function to train the NN over a fixed number of epochs (an epoch is one iteration over the whole training dataset).\n",
    "The function also prints the performance of both training and development/validation set after each epoch.\n",
    "\n",
    "Here we use binary cross‑entropy loss, which is the standard loss function for binary classification tasks such as logistic regression. The module `nn.BCELoss()` expects the model to output probabilities (after a sigmoid), so we do not need to implement any additional activation inside the loss function.\n",
    "\n",
    "Binary cross entropy Loss: https://docs.pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "\n",
    "The optimizer object implements a particular algorithm for updating the weights. Here, we will use the Adam optimizer, which is a variant of stochastic gradient descent method that tends to find a better solution in a smaller number of iterations than standard SGD.\n",
    "\n",
    "Optimization: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "The cell below defines a training function for our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c006a95-1e91-44b4-af5a-8d8c501c69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PerceptronTorch(input_dim=Xtr.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e49b0-7ad1-4573-bd57-4af0ba2f9c07",
   "metadata": {},
   "source": [
    "We need to convert our data into PyTorch‑compatible tensors so the model can train on it. PyTorch cannot operate directly on NumPy arrays — it requires tensors, which are optimized for automatic differentiation and efficient computation on both CPUs and GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42375a1-d99c-4bcf-9df7-3b885bf28d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n",
    "Ytr_t = torch.tensor(Ytr, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "Xtest_t = torch.tensor(Xtest, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42c20c-9912-4adf-b868-9690149df3ed",
   "metadata": {},
   "source": [
    "### Train the Perceptron \n",
    "\n",
    "This training loop does the following:\n",
    "\n",
    "- Runs for a fixed number of epochs so the model repeatedly sees the training data.\n",
    "- Performs a forward pass on the training set to compute predictions.\n",
    "- Calculates the training loss and stores it for plotting.\n",
    "- Backpropagates the error and updates the model’s weights using the optimizer.\n",
    "- Evaluates the model on the test set (in eval mode and without gradients).\n",
    "- Records the validation loss to track generalisation.\n",
    "\n",
    "Plots both curves so you can compare training vs. validation loss over time.\n",
    "\n",
    "**TO-DO:**  Train the network for 30 epochs and plot the losses by completing the cell below. At which epoch did we get the best model fit? How could we use the dev set losses to return the best model? Remember that neural networks tend to overfit if trained too long, as they have many parameters and are very flexible. \n",
    "\n",
    "Note that the answer can vary each time you run the training process due to random initialisation of the model weights and shuffling of the dataset. \n",
    "\n",
    "ANSWER\n",
    "   * The plot shows the best fit around ... epochs before it starts to overfit, as the validation loss converges while training loss goes down \n",
    "   * If dev set loss stops going down for a long time, but training set loss keeps decreasing, the model may be overfitting.\n",
    "   * We can stop training at the point where dev set loss stops decreasing \n",
    "   * Or we could save the model with best dev set performance and use that model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8afc3-56b7-409b-a737-200e8978cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=30\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "\n",
    "### TRAINING LOOP WITH LOSS TRACKING ###\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(Xtr_t)\n",
    "    loss = criterion(outputs, Ytr_t)\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # backward + update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/num_epochs, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # compute dev/validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dev_out = model(Xtest_t)\n",
    "        dev_loss = criterion(dev_out, torch.tensor(Ytest, dtype=torch.float32).view(-1,1))\n",
    "        dev_losses.append(dev_loss.item())\n",
    "\n",
    "### PLOT ###\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(dev_losses, label='dev')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Dev Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b5702-ffe3-4f89-af6d-e2e1ea72771e",
   "metadata": {},
   "source": [
    "### Predict on Test Data\n",
    "Now we can run the model in evaluation mode, generate predicted probabilities for the test set, and convert those probabilities into binary class labels by applying a 0.5 threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3cd8ed-6cad-48dc-b7e7-1c67229ee64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred_prob = model(Xtest_t)\n",
    "    Y_pred = (y_pred_prob >= 0.5).float().numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5cee6-fc8a-4c48-b0ab-179e8f72a4a5",
   "metadata": {},
   "source": [
    "## Part (e) \n",
    "Use the built in metrics in sklearn to calculate the accuracy of both classifiers on the Testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa830d8-0d94-4ca1-90ba-7dec2456f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "##TODO## Write your answer here\n",
    "#Evaluate the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a74805d-41eb-4aea-9f1c-84560a24e564",
   "metadata": {},
   "source": [
    "If the accuracy is low, consider increasing the maximum number of iterations and repeating the fitting and testing process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a187fb-6d88-4e08-ab2d-bfe6ccbc7967",
   "metadata": {},
   "source": [
    "## Part(f) \n",
    "Plot the confusion matrix to visualise the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e305f-26d9-45f6-ac3b-16f887e65cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "##TODO## Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe8233-9265-4aae-9405-b464c721db46",
   "metadata": {},
   "source": [
    "# 2. Multilayer Perceptron\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ematm0067/2025_26/6e01ac01c1e0e85ad7e89176a16e8facc13a199a/worksheets/source/Picture1.png\" width=\"600\" />\n",
    "\n",
    "The input layer, located on the far left, contains neurons that correspond to the input features. Each neuron in the hidden layer processes the values from the previous layer through a weighted sum, which is then passed through a non-linear activation function, such as `ReLU`. Finally, the output layer takes the values from the last hidden layer and converts them into the model’s output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca99fe4-c9d7-44bc-b65f-7ce1da1eeb55",
   "metadata": {},
   "source": [
    "# 2.1. Multi‑class Classification using a Multilayer Perceptron\n",
    "Now that you’ve compared your Perceptron implementation with the torch Perceptron for binary classification, you will extend your work to a multiclass classification task using the load_digits dataset.\n",
    "This dataset contains 8×8 grayscale images of handwritten digits (0–9), represented as 64 numerical pixel‑intensity features.\n",
    "\n",
    "In this task, you will implement a Multilayer Perceptron (MLP) using PyTorch.\n",
    "\n",
    "(a) Load the dataset\n",
    "Use `sklearn.datasets.load_digits()` to obtain the feature matrix and target labels.\n",
    "Store them in appropriate variables.\n",
    "\n",
    "(b) Split the data\n",
    "Use train_test_split to divide the dataset into training and test sets.\n",
    "Convert both splits into PyTorch tensors or wrap them in TensorDataset + DataLoader.\n",
    "\n",
    "(c) Build and train a PyTorch MLP\n",
    "Create a neural network with at least:\n",
    "- an input layer\n",
    "- one or more hidden layers (choose `relu` as activation function)\n",
    "- an output layer of size 10 (one per digit class)\n",
    "\n",
    "\n",
    "Train the model for several epochs and record the training loss.\n",
    "\n",
    "(d) Evaluate your model’s performance on the test set (e.g., accuracy, precision).  \n",
    "(e) Modify your hyperparameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424681ee-6c95-49b6-8f2b-e1bdcddab78e",
   "metadata": {},
   "source": [
    "## Part (a)  \n",
    "From `sklearn.datasets` load the load_digits dataset (function is `load_digits()`). Save the data into a variable `X1` and the targets into a variable `Y1`.  \n",
    "Take a look at the data in `X1`. How many datapoints are there? How many features does each datapoint have? (Hint: use `np.shape`). \n",
    "Take a look at the targets. How many classes does the output need to be classified into??  \n",
    "Look at the values of `X1`. Does the data need to be normalised?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b645ab-6be3-4168-9d66-4f516d4204c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO#  #Load the dataset\n",
    "digits=##TODO##\n",
    "X1 = ##TODO##\n",
    "Y1 = ##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d018e2-6bc9-41f2-930d-c551ac68e96d",
   "metadata": {},
   "source": [
    "Look at the images corresponding to the input. Set the value of the target as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231f87e9-9096-4796-b139-571d73dff51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 10)\n",
    "for i in range(20):\n",
    "    axes[i//10, i %10].imshow(digits.images[i], cmap='gray');\n",
    "    axes[i//10, i %10].axis('off')\n",
    "    axes[i//10, i %10].set_title(f\"PR: {digits.target[i]}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749671d-8675-42b3-b67f-a00c480d31c9",
   "metadata": {},
   "source": [
    "## Part (b)  \n",
    "Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36fbb6f-3928-4c49-a8ed-4edddebd0802",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO##\n",
    "Xtr1, Xtest1, Ytr1, Ytest1=##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e5641-c0e6-497a-bbb9-ade5652a5feb",
   "metadata": {},
   "source": [
    "## Part (c) \n",
    "Define a multilayer perceptron (MLP) with two hidden layers. Each hidden layer should apply a linear transformation followed by a ReLU activation, and the final linear layer should produce the output. Use `nn.Sequential` to connect the layers and implement the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b807792-bcb1-43c7-b5a8-67ea9d268fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Class(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=10, output_dim=10):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, output_dim)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc091e83-88c7-4726-b760-13688f34b618",
   "metadata": {},
   "source": [
    "Set up the MLP by specifying the input size, number of output classes, and the hidden layer sizes. Then create the model and prepare it for training by choosing an appropriate loss function (CrossEntropyLoss for multi‑class classification) and an optimizer (Adam with a learning rate of 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41027082-3978-424e-a0cd-a2bdaa6a27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = Xtr1.shape[1]\n",
    "output_dim = len(torch.unique(torch.tensor(Ytr1)))\n",
    "hidden_layer_sizes=10\n",
    "model1 = MLP_Class(input_dim, hidden_layer_sizes, output_dim)\n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086c44a-e8ec-48f6-a750-5c200872e591",
   "metadata": {},
   "source": [
    "As before, we need to convert our data into PyTorch‑compatible tensors so the model can train on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f4598-01fc-4001-a127-497a5dfa885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_t1 = torch.tensor(Xtr1, dtype=torch.float32)\n",
    "Ytr_t1 = torch.tensor(Ytr1, dtype=torch.long)\n",
    "Ytest_t1 = torch.tensor(Ytest1, dtype=torch.long)\n",
    "\n",
    "Xtest_t1 = torch.tensor(Xtest1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee7cbd-1a0d-4767-b504-00a38488e0f9",
   "metadata": {},
   "source": [
    "Identify the epoch at which the validation loss stops decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b6abbf-861a-477a-ab75-56c34a6caa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=#TODO#\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "    \n",
    "### TRAINING LOOP WITH LOSS TRACKING ###\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    optimizer1.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model1(Xtr_t1)\n",
    "    loss = criterion1(outputs, Ytr_t1)\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # backward + update\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "    print(f\"Epoch {epoch+1}/num_epochs, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # compute dev/validation loss\n",
    "    model1.eval()\n",
    "    with torch.no_grad():\n",
    "        dev_out = model1(Xtest_t1)\n",
    "        dev_loss = criterion1(dev_out, Ytest_t1)\n",
    "        dev_losses.append(dev_loss.item())\n",
    "\n",
    "### PLOT ###\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(dev_losses, label='dev')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Dev Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6c498-cf14-4dc4-b0fd-71d384fa9030",
   "metadata": {},
   "source": [
    "### Predict on Test Data\n",
    "Run the model in evaluation mode, compute the output logits for the test set, and convert these logits into class predictions by taking the index of the largest value (the class with the highest score). Then compare these predicted class labels with the true labels to calculate the model’s accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0537d0-6264-4ebe-a7d3-d234991035f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model1(Xtest_t1)\n",
    "    Y_pred1 = torch.argmax(logits, dim=1).numpy()\n",
    "    \n",
    "##TODO##\n",
    "accuracy_mlp1  ##TODO##\n",
    "print(f\"Accuracy of MLP: {accuracy_mlp1 * 100:.2f}%\")\n",
    "print(\"Classification Report :\")\n",
    "##TODO##\n",
    "print(\"Confusion Matrix:\")\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5a1af-3f0f-43e8-8bf5-59db872f84f3",
   "metadata": {},
   "source": [
    "Visualize a sample of images and their predictions for MLP. Check if it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeaa381-68af-4b4d-bd04-7ad94d5d037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f756ff3-4f52-4fc6-8394-6309fff7f3c3",
   "metadata": {},
   "source": [
    "## Part (e) Experimenting with Hyperparameters\n",
    "Experiment with Hidden Layer Configurations\n",
    "\n",
    "This neural network currently has 2 hidden layers, each with 10 neurons. This is defined using `nn.Linear layers`, for example:`nn.Linear(10, 10)`. \n",
    "- Try experimenting with different configurations of hidden layers (e.g., fewer or more layers, or varying the number of neurons per layer) to observe their effect on the model's performance.\n",
    "- Does increasing the number of layers or neurons always improve the performance? Why or why not?\n",
    "\n",
    "In PyTorch, you choose activation functions manually (e.g., `nn.ReLU()`, `nn.Tanh()`, `nn.Sigmoid()`).\n",
    "- Change the activation function and observe how it impacts the model's performance.\n",
    "\n",
    "In PyTorch, the learning rate is set in the optimizer.\n",
    "- Investigate the Learning Rate. What does increasing or decreasing the value of learning_rate result in?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d006c-19e1-43e5-acbe-6e89d3f7bf84",
   "metadata": {},
   "source": [
    "# 2.2. Regression Using a Multilayer Perceptron in PyTorch\n",
    "In this section, you will build and train a Multilayer Perceptron (MLP) in PyTorch to perform a regression task on the `load_diabetes` dataset. The goal is to predict a continuous target value representing diabetes progression, using 10 numerical input features.\n",
    "\n",
    "In this task, you will:  \n",
    "(a) Load the dataset using `load_diabetes` from `sklearn.datasets`.  \n",
    "(b) Split the data into training and test sets.  \n",
    "(c) Build and train a regression MLP in PyTorch using fully connected layers and an appropriate activation function (e.g., ReLU).  \n",
    "(d) Evaluate the model’s performance on the test set using regression metrics such as Mean Squared Error (MSE) and R² score.  \n",
    "(e) Experiment with different hyperparameters (hidden layer sizes, learning rate, number of epochs) to improve performance.  \n",
    "(f) Plot predicted vs. actual values to visualise how well the model fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e6af1b-95f7-4d59-87e9-bb3295ae2c9c",
   "metadata": {},
   "source": [
    "## Part (a)\n",
    "Load the load_diabetes from sklearn.datasets\n",
    "Check the X and y of your data\n",
    "Take a look at the data in `X2` and the target labels in `Y2`. Find their shapes using `.shape`. \n",
    "- How many data points are there in `X2`?\n",
    "- How many features does each data point have?\n",
    "- Does the data require scaling or normalising before training a neural network model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d15a70-277a-4ab8-9f04-ad9eb5533a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO# Load dataset\n",
    "[X2,Y2]=##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02ef63-66ae-49f3-816d-c038a6446da7",
   "metadata": {},
   "source": [
    "## Part (b) \n",
    "Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510ecbd-6c3f-4bc4-a158-acfe53a8818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO## Split dataset\n",
    "Xtr2, Xtest2, Ytr2, Ytest2 = ##TODO##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed89358-c9aa-4f3d-a96b-1f16a8515768",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "Xtr2 = scaler_X.fit_transform(Xtr2)\n",
    "Xtest2 = scaler_X.transform(Xtest2)\n",
    "\n",
    "scaler_Y = StandardScaler()\n",
    "Ytr2 = scaler_Y.fit_transform(Ytr2.reshape(-1,1))\n",
    "Ytest2 = scaler_Y.transform(Ytest2.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b024df-b5d3-4702-86cb-dcb8337ab55e",
   "metadata": {},
   "source": [
    "## Part (c)\n",
    "Define a multilayer perceptron (MLP) for a regression task using two hidden layers. Each hidden layer should apply a linear transformation followed by a ReLU activation, and the final linear layer should output a single continuous value. Use `nn.Sequential` to connect the layers and implement the forward pass. The final layer should output a single value for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0b485-d778-4267-b4b2-8437c4049395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Reg(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=10):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "        ###TODO## Create the MLP Regressor model\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c2d6f6-cfbe-4003-9cd0-31154861b298",
   "metadata": {},
   "source": [
    "Set up the MLP by specifying the input size, the hidden layer sizes, and an output size of 1 for continuous regression. Then create the model and prepare it for training by selecting a suitable loss function for regression (such as `nn.MSELoss()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefac852-69ca-470d-ae60-15dcea64bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = Xtr2.shape[1]\n",
    "hidden_layer_sizes=##TODO##\n",
    "model2 = MLP_Reg(input_dim, hidden_layer_sizes)\n",
    "criterion2 = ##TODO##\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3473617-e4db-46d0-8175-b26869cdd9b2",
   "metadata": {},
   "source": [
    "As before, we need to convert our data into PyTorch‑compatible tensors so the model can train on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c3d6b-7619-4eee-b2bb-06679a324926",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_t2 = torch.tensor(Xtr2, dtype=torch.float32)\n",
    "Ytr_t2 = torch.tensor(Ytr2, dtype=torch.float32).view(-1, 1)\n",
    "Ytest_t2 = torch.tensor(Ytest2, dtype=torch.float32).view(-1, 1)\n",
    "Xtest_t2 = torch.tensor(Xtest2, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59796dd9-7a9b-4c91-a8f0-424f3a80f49f",
   "metadata": {},
   "source": [
    "Identify the epoch at which the validation loss stops decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88118ae5-0324-418a-be2d-34322235d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=##TODO##\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "    \n",
    "### TRAINING LOOP WITH LOSS TRACKING ###\n",
    "for epoch in range(num_epochs):\n",
    "    model2.train()\n",
    "    optimizer2.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model2(Xtr_t2)\n",
    "    loss = criterion2(outputs, Ytr_t2)\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # backward + update\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # compute dev/validation loss\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        dev_out = model2(Xtest_t2)\n",
    "        dev_loss = criterion2(dev_out, Ytest_t2)\n",
    "        dev_losses.append(dev_loss.item())\n",
    "\n",
    "### PLOT ###\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(dev_losses, label='dev')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Dev Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0eeadd-61fe-4d03-b8a2-3c04e76bb13e",
   "metadata": {},
   "source": [
    "## Part (d)\n",
    "Make predictions on the test set (`Xtest2`).\n",
    "Evaluate the model using appropriate metrics from `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ed5aa-2131-4d30-b822-e9c17412172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred2 = model2(Xtest_t2).numpy().flatten()\n",
    "\n",
    "# TODO: Evaluate regression performance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = #TODO##\n",
    "r2 = ##TODO##\n",
    "print(f\"MSE of MLP: {mse:.4f}\")\n",
    "print(f\"R² Score of MLP: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab638f-f8b4-4f78-b4ac-a2c85658a383",
   "metadata": {},
   "source": [
    "## Part (e) Experimenting with Hyperparameters\n",
    "Experiment with Hidden Layer Configurations\n",
    "\n",
    "This neural network currently has 2 hidden layers, each with 10 neurons. This is defined using `nn.Linear layers`, for example:`nn.Linear(10, 10)`. \n",
    "- Try experimenting with different configurations of hidden layers (e.g., fewer or more layers, or varying the number of neurons per layer) to observe their effect on the model's performance.\n",
    "- Does increasing the number of layers or neurons always improve the performance? Why or why not?\n",
    "\n",
    "In PyTorch, you choose activation functions manually (e.g., `nn.ReLU()`, `nn.Tanh()`, `nn.Sigmoid()`).\n",
    "- Change the activation function and observe how it impacts the model's performance.\n",
    "\n",
    "In PyTorch, the learning rate is set in the optimizer.\n",
    "- Investigate the Learning Rate. What does increasing or decreasing the value of learning_rate result in?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802dd82-4a0b-4193-acec-d0761f8e5cda",
   "metadata": {},
   "source": [
    "## Part (f)\n",
    "Plot the predicted vs actual values\n",
    "After training the `MLP_Reg` and predicting the outputs for `Xtest2`, create a scatter plot to compare the predicted values (`y_pred2`) against the actual values (`Ytest2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a860c-01ca-4cbe-9269-cda8a33972cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO# plot the predicted vs actual values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
